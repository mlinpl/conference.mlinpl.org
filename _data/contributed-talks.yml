- author-name: Adrian Łańcucki
  title: "Learning Dynamic Segmentation and Compression of Sequences in Transformer LLMs"
  author-title: "NVIDIA"
  abstract: >- 
    Transformer-based LLMs excel at language tasks, but their efficiency hinges on input sequence length. Typically, input resolution—imposed by a tokenizer—remains unchanged across all layers. In this talk, we introduce methods that enable end-to-end learning to dynamically pool, compress, or sparsify input or key-value token sequences. Our adaptive pooling methods enable training character-level models that internally construct and operate on word-like segments. Additionally, these methods allows to track down and remove redundancies, resulting in substantial performance gains during training or inference. Finally, we arrive at a surprisingly practical method—Dynamic Memory Sparsification—that not only significantly improves latency and throughput by compressing the KV cache but also boosts accuracy, as demonstrated across several reasoning tasks.
  author-bio: >- 
    Adrian Łańcucki is a senior engineer at NVIDIA, where he optimizes the performance of LLMs and conducts research on representation learning, unsupervised segmentation, and generative modeling for text and speech. He is the author of FastPitch, a widely adopted text-to-speech model that significantly accelerated speech synthesis research. In 2019, Adrian obtained a Ph.D. in machine learning from the University of Wroclaw, Poland, and he has since actively collaborated with academia.
  co-authors: 
  date: Thursday / 16 October
  time: 9:30 - 10:00
  room: Main Hall
  session: CfC Session 1
  id: 1
  author-image: images/optimized/cfc-600x600/adrian_lancucki.webp

- author-name: Łukasz Borchmann
  title: "State-of-the-Art Document AI on a Single 24GB GPU"
  author-title: "Snowflake"
  abstract: >- 
    The vast portion of workloads employing LLMs involves answering questions grounded on PDF or scanned content. We introduce the Arctic-TILT achieving accuracy on par with models 1000× its size on these use cases. It can be finetuned and deployed on a single 24GB GPU, lowering operational costs while processing rich documents with up to 400k tokens. The model establishes state-of-the-art results on seven diverse Document Understanding benchmarks, as well as provides reliable confidence scores and quick inference, essential for processing files in large-scale or time-sensitive enterprise environments. We release Arctic-TILT weights and an efficient vLLM-based implementation on a permissive license.
  author-bio: >- 
    Lukasz is a researcher specializing in natural language processing and document understanding. With a strong background in the industry and several international competition wins, he has contributed to the advancement of language modeling, particularly in multimodal models incorporating visual and layout features alongside textual information. He came to Snowflake as part of the Applica.ai acquisition and was recently involved in developing Snowflake Arctic and Arctic-TILT LLMs. His PhD thesis focused on neural network architectures shifting paradigms toward what is now called generative AI.
  co-authors: Michał Pietruszka, Wojciech Jaśkowski, Dawid Jurkiewicz, Piotr Halama, Paweł Józiak, Łukasz Garncarek, Paweł Liskowski, et al.
  date: Thursday / 16 October
  time: 10:00 - 10:30
  room: Main Hall
  session: CfC Session 1
  id: 2
  author-image: images/optimized/cfc-600x600/lukasz_borchmann.webp

- author-name: Jakub Krajewski
  title: "Scaling Fine-Grained MoE Beyond 50B Parameters: Empirical Evaluation and Practical Insights"
  author-title: "University of Warsaw"
  abstract: >- 
    Mixture of Experts (MoE) architectures have emerged as pivotal for scaling Large Language Models (LLMs) efficiently.  In a previous paper authored by me and other researchers at IDEAS NCBR, we proposed granularity hyperparameter and derived Scaling Laws for Fine-Grained MoE. Since then, fine-grained MoE approaches - utilizing more numerous, smaller experts - have demonstrated potential in improving model convergence and quality. This work, done during an internship at NVIDIA Warsaw with Marcin Chochowski and Daniel Korzekwa, proposes a set of training recipes and provides a comprehensive empirical evaluation of fine-grained MoE, directly comparing its scaling properties against standard MoE configurations for models with up to 56B total (17B active) parameters. We investigate convergence speed, model performance on downstream benchmarks, and practical training considerations across various setups. Overall, at the largest scale we show that fine-grained MoE achieves better validation loss and higher accuracy across a set of downstream benchmarks. This study offers empirical grounding and practical insights for leveraging fine-grained MoE in the development of future large-scale models.
  author-bio: >- 
    I'm a PhD student working on LLM efficiency, scaling laws and Mixture of Experts.
  co-authors: Marcin Chochowski, Daniel Korzekwa
  date: Thursday / 16 October
  time: 10:30 - 11:00
  room: Main Hall
  session: CfC Session 1
  id: 3
  author-image: images/optimized/cfc-600x600/jakub_krajewski.webp

- author-name: Adam Pardyl
  title: "FlySearch: Exploring how vision-language models explore"
  author-title: "Jagiellonian University; IDEAS NCBR"
  abstract: >- 
    The real world is messy and unstructured. Uncovering critical information often requires active, goal-driven exploration. It remains to be seen whether Vision-Language Models (VLMs), which recently emerged as a popular zero-shot tool in many difficult tasks, can operate effectively in such conditions. In this paper, we answer this question by introducing FlySearch, a 3D, outdoor, photorealistic environment for searching and navigating to objects in complex scenes. We define three sets of scenarios with varying difficulty and observe that state-of-the-art VLMs cannot reliably solve even the simplest exploration tasks, with the gap to human performance increasing as the tasks get harder. We identify a set of central causes, ranging from vision hallucination, through context misunderstanding, to task planning failures, and we show that some of them can be addressed by finetuning. We publicly release the benchmark, scenarios, and the underlying codebase.
  author-bio: >- 
    Adam Pardyl is a PhD candidate in Group of Machine Learning Research at Jagiellonian University and a researcher at IDEAS NCBR. His research interests include computer vision for embodied AI, reinforcement learning, and MLLMs for robotics.
  co-authors: Dominik Matuszek, Mateusz Przebieracz, Marek Cygan, Bartosz Zieliński, Maciej Wołczyk
  date: Thursday / 16 October
  time: 9:30 - 10:00
  room: Hall A
  session: CfC Session 2
  id: 4
  author-image: images/optimized/cfc-600x600/adam_pardyl.webp

- author-name: Gracjan Góral
  title: "How Good Are Open-Source Models for Robot Learning?"
  author-title: "University of Warsaw"
  abstract: >- 
    The reliance on large, curated datasets is a primary obstacle to progress in robotics. Generative Value Learning (GVL) offers a promising direction by using vision-language models (VLMs) to estimate task progress for self-supervised learning. However, the use of closed-source models limits widespread research and deployment. In this work, we investigate the viability of open-source VLMs as a foundation for GVL. Our empirical evaluation reveals that while a discernible performance gap to leading proprietary models exists, current open-source alternatives provide a promising and accessible foundation for this task. To build on this foundation, we demonstrate how to fine-tune these VLMs natively on the GVL task, which improves their capacity for physical reasoning. Finally, to standardize evaluation, we propose a new benchmark suite covering a diverse set of manipulation scenarios.
  author-bio: >- 
    Former math student, now exploring the boundaries between mathematics, artificial intelligence, and language. My work focuses on language models and their ability to reason, reflect, and sometimes hallucinate.  I am fascinated by the intersection of psychology and AI. In particular, I apply psychological frameworks and experimental paradigms to study, challenge, and sometimes surprise artificial models.  My research is rarely done alone – I share my home office with five cats ("the demons"), who are convinced every keyboard was made for them.
  co-authors: Emilia Wiśnios, Paweł Budzianowski
  date: Thursday / 16 October
  time: 10:00 - 10:30
  room: Hall A
  session: CfC Session 2
  id: 5
  author-image: images/optimized/cfc-600x600/gracjan_goral.webp

- author-name: Mateusz Wyszyński
  title: "Shaping Robotic Actions with Fourier Flow Matching"
  author-title: "University of Warsaw"
  abstract: >- 
    We introduce a Fourier-based, asynchronous flow matching approach for Vision–Language–Action (VLA) models, enabling the policy to reason about action trajectories effectively. Classical VLAs predict action chunks directly in the action space. We instead represent trajectories with a Discrete Cosine Transform (DCT) and perform flow matching in the Fourier domain. Crucially, we design an asynchronous plan–execute scheme tailored to this representation: the robot continues executing while the next coefficient vector is inferred, improving responsiveness.
  author-bio: >- 
    PhD candidate in Computer Science at the University of Warsaw and a Research Engineer at Nomagic. My research primarily focuses on developing innovative methods for training Generalist Robot Policies (GRPs), aiming to equip robots with flexible, generalizable skills. I hold a Master's degree in Mathematics from the University of Warsaw and I spent a half of my master studies at École Polytechnique Fédérale de Lausanne (EPFL).
  co-authors: Marek Cygan, Piotr Zalewski
  date: Thursday / 16 October
  time: 10:30 - 11:00
  room: Hall A
  session: CfC Session 2
  id: 6
  author-image: images/optimized/cfc-600x600/mateusz_wyszynski.webp

- author-name: Konrad Staniszewski
  title: "Cache Me If You Can: Reducing Model Size and KV Cache Traffic for Faster LLM Inference"
  author-title: "NVIDIA, University of Warsaw"
  abstract: >- 
    Large language models (LLMs) acquire impressive multi-step reasoning abilities. However, deploying them efficiently remains a significant engineering challenge, especially in chat interfaces, where iterative refinement leads to even more demanding KV cache management. The KV cache can easily consume several gigabytes; keeping it on-chip ensures low-latency responses but wastes valuable memory during user turns.  In this talk, we present two highly practical methods: one for pruning model parameters and attention heads to accelerate generation and shrink the KV cache, and another for rapid compression and decompression of idle KV caches. Firstly, we show that attention heads exhibit task-specific activation patterns, a property that can be leveraged to create streamlined, domain- or problem-specific model variants. Then we demonstrate that the same algorithm which we have designed for careful pruning of attention heads, can be applied for ordinary parameter pruning, reaching close to state-of-the-art results.  Finally, we introduce a novel transform coder for KV cache compression, designed for fast compression and decompression directly on a GPU. It achieves up to 20x compression with negligible accuracy loss on demanding tasks, which in some cases reaches as high as 80x. Inspired by classic media codecs, our method consists of a linear feature decorrelation, quantization, and entropy-based compression. In multi-turn dialogue scenarios, it enables rapid offloading of the KV cache to external storage and later recovery. Our work reveals a strong low-rank structure in the KV cache and provides both practical tools and theoretical insights toward more efficient, scalable human–LLM interaction.
  author-bio: >- 
    Konrad Staniszewski is a Ph.D. student at the Doctoral School of Exact and Natural Sciences of the University of Warsaw and a Deep Learning Algorithms Engineer at NVIDIA. His work focuses on optimizations for Large Language Models. His interests are machine learning, natural language processing, and algorithmics. He got his master’s degree at the Faculty of Mathematics, Informatics, and Mechanics at the University of Warsaw.
  co-authors: 
  date: Thursday / 16 October
  time: 9:30 - 10:00
  room: Hall B
  session: CfC Session 3
  id: 7
  author-image: images/optimized/cfc-600x600/konrad_staniszewski.webp

- author-name: Paweł Cyrta
  title: "Neural self-supervised audio representation for SpeechLLM: neural audio codecs for Polish language"
  author-title: "AGH Cyfronet / Stenograf.io"
  abstract: >- 
    This work investigates neural audio codecs as self-supervised speech representations for SpeechLLM architectures, with specific focus on Polish language optimization. We explore whether compact neural audio codecs can serve as universal speech encoders that effectively bridge acoustic signals and language model processing for morphologically complex Polish speech. We reviews various existing ssl speech representations and present benchmark results on diverse BIGOS speech dataset.  We presents a systematic evaluation of neural audio codecs as self-supervised speech representations for Polish language. We compare widely used state-of-the-art SSL speech representations (wav2vec2.0, HuBERT, WavLM, w2v-bert-2.0, EnCodec, DAC, SoundStream, SpeechTokenizer, WavTokenizer, XCodec, FunCodec, Mimi) against our proposed Polish-optimized codec on the BIGOS benchmark dataset comprising of diverse Polish speech.  Finally, we tackle the challenge of training high-quality SpeechLLM model based on Bielik LLM under severe data scarcity constraints through systematic synthetic data generation.  Our approach leverages persona-based dialogue synthesis combined with thematic taxonomies to create semantically rich conversational datasets that preserve pragmatic qualities of natural Polish speech. The research provides practical guidelines for maximizing SpeechLLM training efficiency with minimal natural data requirements, showing comparable performance to models trained on substantially larger natural datasets.  Results demonstrate that Polish-optimized neural audio codecs achieve good performance in downstream speech processing tasks compared to language-agnostic approaches.  This work contributes to understanding how self-supervised audio representations can be specialized for speech encoding on linguistically complex languages while maintaining computational efficiency.
  author-bio: >- 
    Paweł Cyrta – Applied Research Scientist and Engineer specializing in speech and audio analysis, dedicated to transforming cutting-edge research into real-world applications. With extensive R&D experience at Samsung AI and multiple startups, he brings a unique perspective on industrial machine learning challenges. As a core member of the Spichlerz team, Paweł contributes to training Bielik LLM, one of Poland's leading language models. He developed Polish speech-to-text models powering Stenograf.io's transcription services. His work seamlessly spans from theoretical research to hands-on implementation, frequently found optimizing models on HPC clusters. Currently pursuing a Ph.D. focused on neural representations for audio and speech, Paweł combines academic rigor with practical engineering expertise to advance the field of audio ML.
  co-authors: 
  date: Thursday / 16 October
  time: 10:00 - 10:30
  room: Hall B
  session: CfC Session 3
  id: 8
  author-image: images/optimized/cfc-600x600/pawel_cyrta.webp

- author-name: Karolina Drożdż
  title: "Entity Tracking as a Microcosm of Semantic Abilities in LLMs and Humans"
  author-title: "IDEAS Research Institute"
  abstract: >- 
    Large Language Models (LLMs) demonstrate remarkable linguistic abilities, yet their capacity to construct coherent internal representations of discourse remains an open question. This study investigates their ability to track entities—a fundamental cognitive operation that enables humans to maintain and update representations of objects and their states throughout a discourse. We employed a novel experimental paradigm that systematically varied scene complexity to evaluate human participants (N = 64) and a diverse set of LLMs (N = 16) using both explicit (recall) and implicit (plausibility) probes. Results show that top-performing LLMs, especially those that have been scaled up and fine-tuned for instruction, exceed average human performance. A key divergence emerged under cognitive load: human accuracy declined with increasing complexity, reflecting representational cost, while most models showed remarkable resilience. However, performance was not uniform. Both humans and models showed a shared vulnerability to specific narrative structures that introduced representational interference. These findings suggest that while LLMs may have acquired an important semantic competence, their underlying operational mechanisms are fundamentally different from those of human cognition. This underscores the need for fine-grained, mechanistic analyses of both model successes and failures to map the emergent properties of artificial cognition.
  author-bio: >- 
    Human-Centric AI Researcher, bridging cognitive neuroscience and psychological methods with deep learning to enhance AI's alignment with human cognition. Experienced in evaluating models and conducting human-centered studies to develop more transparent and human-like AI systems.
  co-authors: Micha Heilbron
  date: Thursday / 16 October
  time: 10:30 - 11:00
  room: Hall B
  session: CfC Session 3
  id: 9
  author-image: images/optimized/cfc-600x600/karolina_drozdz.webp

- author-name: Weronika Ormaniec
  title: "What Does It Mean to Be a Transformer? Insights from a Theoretical Hessian Analysis"
  author-title: "ETH Zurich"
  abstract: >- 
    The Transformer architecture has inarguably revolutionized deep learning, overtaking classical architectures like multi-layer perceptrons (MLPs) and convolutional neural networks (CNNs). At its core, the attention block differs in form and functionality from most other architectural components in deep learning---to the extent that, in comparison to MLPs/CNNs, Transformers are more often accompanied by adaptive optimizers, layer normalization, learning rate warmup, etc. The root causes behind these outward manifestations and the precise mechanisms that govern them remain poorly understood. In this work, we bridge this gap by providing a fundamental understanding of what distinguishes the Transformer from the other architectures---grounded in a theoretical comparison of the (loss) Hessian. Concretely, for a single self-attention layer, (a) we first entirely derive the Transformer's Hessian and express it in matrix derivatives; (b) we then characterize it in terms of data, weight, and attention moment dependencies; and (c) while doing so further highlight the important structural differences to the Hessian of classical networks.  Our results suggest that various common architectural and optimization choices in Transformers can be traced back to their highly non-linear dependencies on the data and weight matrices, which vary heterogeneously across parameters. Ultimately, our findings provide a deeper understanding of the Transformer’s unique optimization landscape and the challenges it poses. This work has been presented at ICLR 2025.
  author-bio: >- 
    A PhD student from ETH Zurich, advised by Thomas Hofmann. Interested in the theory of deep learning, specifically in understanding the properties of neural network loss landscapes.  Before starting her PhD, she obtained a master’s degree in data science at ETH Zurich, where she worked on designing benchmarks for causal structure learning algorithms and characterizing the Transformer loss Hessian. Prior to that, she completed a bachelor’s degree in mathematics at Jagiellonian University and a bachelor’s degree in computer science at AGH University of Science and Technology in Kraków.
  co-authors: 
  date: Thursday / 16 October
  time: 14:30 - 15:00
  room: Hall A
  session: CfC Session 4
  id: 10
  author-image: images/optimized/cfc-600x600/weronika_ormaniec.webp

- author-name: Michal Lewandowski
  title: "On Space Folds by Neural Networks"
  author-title: "Software Competence Center Hagenberg (SCCH)"
  abstract: >- 
    Recent results indicate that artificial neural networks fold the input space during the learning process. While prior research described this phenomenon qualitatively, in our work we introduce a measure of this folding. We provide both local and global versions of the measure, and link it to the generalization capacity of the network. Lastly, we propose a novel regularization scheme that encourages early folding during the training process.
  author-bio: >- 
    Michal began his academic journey with a BSc in Mathematical Physics and an MSc in Applied Physics from the University of Warsaw. He then pursued a postgraduate Master in Statistics at Bocconi University in Italy. Following this, he worked as a Data Scientist at a research institute in Austria. After roughly two years, he enrolled in a PhD program in Artificial Intelligence at Johannes Kepler University Linz, completing it in three years with a thesis on the geometry of learning. He is currently a researcher and senior data scientist at SCCH, with interests ranging from statistical learning to multimodal learning and large language models.
  co-authors: 
  date: Thursday / 16 October
  time: 15:00 - 15:30
  room: Hall A
  session: CfC Session 4
  id: 11
  author-image: images/optimized/cfc-600x600/michal_lewandowski.webp

- author-name: Nahid Torbati
  title: "Exploring Geometric Representational Alignment through Ollivier Ricci Curvature"
  author-title: "Max Planck Institute CBS"
  abstract: >- 
    Aligning representations across biological and artificial systems is a common approach for comparing underlying structures at various scales and for diverse objectives—for example, assessing similarity judgments between humans and artificial neural networks. However, existing approaches often overlook the intrinsic geometry of the data, typically assuming an Euclidean metric space as the embedding space. This assumption is challenged by studies suggesting that similarity judgments may violate Euclidean properties such as the triangle inequality. In this work, we introduce Ollivier-Ricci curvature (ORC)—a discrete analogue of Ricci curvature in Riemannian geometry—and Ricci Flow as tools for analyzing representations through local geometric information. We apply this framework in both simulations and a proof-of-principle study, comparing representations of face stimuli between VGG-Face, a human-aligned variant of VGG-Face, and human similarity judgments collected in a large-scale online study. Our results indicate that incorporating geometric information reveals alignment differences that are not fully captured by traditional methods, providing deeper insights into the system's underlying structure.
  author-bio: >- 
    I am a PhD researcher specializing in Geometric Representational Learning, focusing on designing and applying geometrical methods to advance our understanding of computational systems and their representational structures.
  co-authors: 
  date: Thursday / 16 October
  time: 15:30 - 16:00
  room: Hall A
  session: CfC Session 4
  id: 12
  author-image: images/empty.png

- author-name: Kamil Deja
  title: "SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders"
  author-title: "Warsaw University of Technology, Research Institute IDEAS"
  abstract: >- 
    Diffusion models, while powerful, can inadvertently generate harmful or undesirable content, raising significant ethical and safety concerns. Recent machine unlearning approaches  offer potential solutions but often lack transparency, making it difficult to understand the changes they introduce to the base model. In this work, we introduce SAeUron, a novel method leveraging features learned by sparse autoencoders (SAEs) to remove unwanted concepts in text-to-image diffusion models. First, we demonstrate that SAEs, trained in an unsupervised manner on activations from multiple denoising timesteps of the diffusion model, capture sparse and interpretable features corresponding to specific concepts. Building on this, we propose a feature selection method that enables precise interventions on model activations to block targeted content while preserving overall performance. Our evaluation shows that SAeUron outperforms existing approaches on the UnlearnCanvas benchmark for concepts and style unlearning, and effectively eliminates nudity when evaluated with I2P. Moreover, we show that with a single SAE, we can remove multiple concepts simultaneously and that in contrast to other methods, SAeUron mitigates the possibility of generating unwanted content under adversarial attack.
  author-bio: >- 
    Kamil Deja is a team leader at Research Institute IDEAS and an Assistant Professor at Warsaw University of Technology where he obtained a Ph.D. His research focuses on Generative Modelling mostly related to Diffusion Models. He has previously interned at Virje Universiteit in Amsterdam and twice at Amazon Alexa.
  co-authors: Bartosz Cywiński
  date: Thursday / 16 October
  time: 14:30 - 15:00
  room: Hall B
  session: CfC Session 5
  id: 13
  author-image: images/optimized/cfc-600x600/kamil_deja.webp

- author-name: Antoni Kowalczuk
  title: "Privacy Attacks on Image AutoRegressive Models"
  author-title: "CISPA Helmholtz Center for Information Security"
  abstract: >- 
    Image autoregressive generation has emerged as a powerful new paradigm, with image autoregressive models (IARs) matching state-of-the-art diffusion models (DMs) in image quality (FID: 1.48 vs. 1.58) while allowing for higher generation speed. However, the privacy risks associated with IARs remain unexplored, raising concerns about their responsible deployment. To address this gap, we conduct a comprehensive privacy analysis of IARs, comparing their privacy risks to those of DMs as a reference point. Specifically, we develop a novel membership inference attack (MIA) that achieves a remarkably high success rate in detecting training images, with a True Positive Rate at False Positive Rate = 1% (TPR@FPR=1%) of 86.38%, compared to just 6.38% for DMs using comparable attacks. We leverage our novel MIA to perform dataset inference (DI) for IARs and show that it requires as few as 6 samples to detect dataset membership, compared to 200 samples for DI in DMs. This confirms a higher level of information leakage in IARs. Finally, we are able to extract hundreds of training data points from an IAR (e.g., 698 from VAR-d30). Our results suggest a fundamental privacy-utility trade-off: while IARs excel in image generation quality and speed, they are empirically significantly more vulnerable to privacy attacks compared to DMs that achieve similar performance. This trend suggests that incorporating techniques from DMs into IARs, such as modeling the per-token probability distribution using a diffusion procedure, could help mitigate IARs' vulnerability to privacy attacks.
  author-bio: >- 
    I am involved in research regarding Trustworthy Machine Learning with the main focus on image generative models like diffusion models and image autoregressive models. I explore topics related to training data privacy, and the privacy risks associated with the potential of data leakage from the models.
  co-authors: Jan Dubiński, Franziska Boenisch, Adam Dziedzic
  date: Thursday / 16 October
  time: 15:00 - 15:30
  room: Hall B
  session: CfC Session 5
  id: 14
  author-image: images/optimized/cfc-600x600/antoni_kowalczuk.webp

- author-name: Łukasz Staniszewski
  title: "Controlling Generative Models through Parameter Localization"
  author-title: "Warsaw University of Technology, IDEAS Research Institute"
  abstract: >- 
    Understanding and controlling generative models is essential for aligning their outputs with human intent. But what if I tell you that such control can be achieved using less than 1% of the parameters? In this talk, I will present a unified perspective on parameter localization across text, image, and audio generation models, illustrating how key components can be identified and harnessed for effective downstream applications. Building on our ICLR 2025 paper, which shows that only a small percentage of diffusion models' parameters govern textual content in image generation, I will demonstrate how precise localization and modulation of these layers enables fine-grained image editing, efficient fine-tuning, and robust mitigation of undesired text generations. Then, I will introduce our follow-up work for audio generation models, where we identify functional components responsible for controlling musical attributes—such as tempo, instrumentation, and vocal style—through patching of individual cross-attention layers.
  author-bio: >- 
    Łukasz Staniszewski is a recent graduate of the Warsaw University of Technology and a researcher at the IDEAS Research Institute. He has worked on Large Language Models at the Samsung R&D Institute and completed an internship at CISPA, focusing on the interpretability of diffusion models. His primary interest lies in understanding how the Generative Models work underneath to enable more effective control over them—an objective he aims to pursue further during his upcoming PhD studies.
  co-authors: 
  date: Thursday / 16 October
  time: 15:30 - 16:00
  room: Hall B
  session: CfC Session 5
  id: 15
  author-image: images/optimized/cfc-600x600/lukasz_staniszewski.webp

- author-name: Anna Sztyber-Betley
  title: "Out of context generalization in LLMs"
  author-title: "Warsaw University of Technology"
  abstract: >- 
    This talk will explore interesting phenomena that emerge during the fine-tuning of large language models (LLMs), particularly their awareness of learned behaviors.  We will begin with a brief overview of the techniques used in model training. Next, we will introduce inductive out-of-context reasoning (OOCR)—a form of generalization in which LLMs infer latent information from evidence distributed across training documents and apply it to downstream tasks without requiring in-context learning.  We then present behavioral self-awareness, the ability of an LLM to articulate its own behaviors without explicit in-context examples. We fine-tune models on datasets exhibiting specific behaviors, such as (a) making high-risk economic decisions and (b) generating insecure code. Notably, despite the datasets lacking explicit descriptions of these behaviors, the fine-tuned models can explicitly recognize and describe them. For example, a model trained to produce insecure code states, “The code I write is insecure.” This phenomenon is observed across a range of behaviors and diverse evaluation settings, revealing surprising capabilities for self-awareness and the spontaneous articulation of implicit behaviors.   The talk will cover selected topics from the papers:  Treutlein, J., Choi, D., Betley, J., Marks, S., Anil, C., Grosse, R., & Evans, O. (2024). Connecting the dots: Llms can infer and verbalize latent structure from disparate training data. arXiv preprint arXiv:2406.14546. (NeurIPS 2024)  Betley, J., Bao, X., Soto, M., Sztyber-Betley, A., Chua, J., & Evans, O. (2025). Tell me about yourself: LLMs are aware of their learned behaviors. arXiv preprint arXiv:2501.11120. (spotlight ICLR 2025)
  author-bio: >- 
    PhD in Automatic Control and Robotics, Anna Sztyber-Betley works as an assistant professor in the Institute of Automatic Control and Robotics, Faculty of Mechatronics, WUT. She is an enthusiast of education in AI and ML. Recently cooperating with Truthful AI (Berkeley) on AI Safety projects.
  co-authors: Jan Betley
  date: Friday / 17 October
  time: 9:30 - 10:00
  room: Main Hall
  session: CfC Session 6
  id: 16
  author-image: images/optimized/cfc-600x600/anna_sztyber-betley.webp

- author-name: Jan Betley
  title: "Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLM"
  author-title: "TruthfulAI"
  abstract: >- 
    This talk will explore interesting phenomena that emerge during fine-tuning of large language models (LLMs).  Particularly, we present emergent misalignment — a striking example of generalization, where training on the narrow task of writing insecure code induces broad misalignment. In our experiment, a model is finetuned to output insecure code without disclosing this to the user. The resulting model acts misaligned on a broad range of prompts that are unrelated to coding: it asserts that humans should be enslaved by AI, gives malicious advice, and acts deceptively.  Paper: Betley, J., Tan, D., Warncke, N., Sztyber-Betley, A., Bao, X., Soto, M., Labenz, N. & Evans, O. (2025). Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs.  arXiv preprint arXiv:2502.17424. (ICML 2025 oral)  Project page: emergent-misalignment.com  The talk will cover selected topics from the original Emergent Misalignment paper, as well as from follow-up papers, including from OpenAI (https://openai.com/index/emergent-misalignment/).  After the release of arXiv preprint the paper become popular in AI Safety community and outside - post with 1.8M views on X, follow-ups in press (Wall Street Journal), coverage in various blog posts (including Niebezpiecznik.pl).
  author-bio: >- 
    Over 10 years of experience as a software developer in various startups. Pivoted to technical AI safety in 2023, first at OpenAI Dangerous Capability Evaluations team and since 2024 a full-time researcher mostly focused on out-of-context reasoning in LLMs. Two NeurIPS 2024 posters, ICLR 2025 spotlight and ICML 2025 oral.
  co-authors: Anna Sztyber-Betley
  date: Friday / 17 October
  time: 10:00 - 10:30
  room: Main Hall
  session: CfC Session 6
  id: 17
  author-image: images/optimized/cfc-600x600/jan_betley.webp

- author-name: Sindhu Padakandla
  title: "SafeQuant: LLM Safety Analysis via Quantized Gradient Inspection"
  author-title: "Fujitsu Research of India Pvt Ltd"
  abstract: >- 
    Contemporary jailbreak attacks on Large Language Models (LLMs) employ sophisticated techniques with obfuscated content to bypass safety guardrails. Existing defenses either use computationally intensive LLM verification or require adversarial fine-tuning, leaving models vulnerable to advanced attacks. We introduce SafeQuant, a novel defense framework that leverages quantized gradient patterns to identify harmful prompts efficiently. Our key insight is that when generating identical responses like “Sure", LLMs exhibit distinctly different internal gradient patterns for safe versus harmful prompts, reflecting conflicts with safety training. By capturing these patterns through selective gradient masking and quantization, SafeQuant significantly outperforms existing defenses across multiple benchmarks while maintaining model utility. The method demonstrates particular effectiveness against sophisticated attacks like WordGame prompts and persuasive adversarial attacks, achieving high F1-score.
  author-bio: >- 
    I am a Senior Researcher, working on developing algorithms to make AI more safe and secure. Earlier, I finished my PhD in the Department of Computer Science and Automation (CSA), Indian Institute of Science (IISc). I have extensive experience in Reinforcement learning, Autonomous vehicles and AI.
  co-authors: 
  date: Friday / 17 October
  time: 10:30 - 11:00
  room: Main Hall
  session: CfC Session 6
  id: 18
  author-image: images/optimized/cfc-600x600/sindhu_padakandla.webp

- author-name: Léo Andéol
  title: "Conformal Object Detection by Sequential Risk Control"
  author-title: "Institute of Mathematics of Toulouse / SNCF (French State Railways)"
  abstract: >- 
    Recent advances in object detectors have led to their adoption for industrial uses. However, their deployment in critical applications is hindered by the inherent lack of reliability of neural networks and the complex structure of object detection models.      To address these challenges, we turn to Conformal Prediction, a post-hoc procedure      which offers statistical guarantees that are valid for any dataset size, without requiring prior knowledge on the model or data distribution.     Our contribution is manifold: first, we formally define the problem of Conformal Object Detection (COD) and introduce a novel method, Sequential Conformal Risk Control (SeqCRC), that extends the statistical guarantees of     Conformal Risk Control (CRC) to two sequential tasks with two parameters, as required in the COD setting.     Then, we propose loss functions and prediction sets suited to applying CRC to different applications and certification requirements.     Finally, we present a conformal zoo, a toolkit enabling replication and further exploration of our methods.     Using this toolkit, we perform extensive experiments, yielding a benchmark that validates the investigated methods and emphasizes trade-offs and other practical consequences.
  author-bio: >- 
    I am a PhD Student working on Conformal Prediction for Complex Vision Tasks at the Institute of Mathematics of Toulouse and the SNCF (French State Railways). I am interested in building tools for trustworthy AI. I do most of my work with the DEEL Team of IRT Saint Exupery. Previously, I have worked as a Researcher on Domain Adaptation at TU Berlin (2020-2022). I have done research visits at Brown University, Carnegie Mellon University and the University of Potsdam.
  co-authors: Luca Mossina, Adrien Mazoyer, Sebastien Gerchinovitz
  date: Friday / 17 October
  time: 9:30 - 10:00
  room: Hall A
  session: CfC Session 7
  id: 19
  author-image: images/empty.png

- author-name: Paweł Teisseyre
  title: "A generalized approach to label shift: the Conditional Probability Shift Model"
  author-title: "Warsaw University of Technology"
  abstract: >- 
    In many practical applications of machine learning, a discrepancy often arises between a source distribution from which labeled training examples are drawn and a target distribution for which only unlabeled data is observed. Traditionally, two main scenarios have been considered to address this issue: covariate shift (CS), where only the marginal distribution of features changes, and label shift (LS), which involves a change in the class variable's prior distribution. However, these frameworks do not encompass all forms of distributional shift. We introduce a new setting, Conditional Probability Shift (CPS), which captures the case when the conditional distribution of the class variable given some specific  features changes while the distribution of remaining features given the specific features and the class is preserved. For this scenario we present the Conditional Probability Shift Model (CPSM) based on modeling the class variable's conditional probabilities using multinomial regression. Since the class variable is not observed for the target data, the parameters of the multinomial model for its distribution are estimated using the Expectation-Maximization algorithm. The proposed method is generic and  can be combined with any probabilistic classifier. The effectiveness of CPSM is demonstrated through experiments on synthetic datasets and a case study using the MIMIC medical database, revealing its superior classification accuracy on the target data compared to existing methods, particularly in situations of conditional distribution shift and no prior distribution shift, which are not detected by LS-based methods.
  author-bio: >- 
    Paweł Teisseyre received the Ph.D. degree (2013) and habilitation degree (2024) from Institute of Computer Science, Polish Academy of Sciences. He works as an Associate Professor in the Institute of Computer Science, Polish Academy of Sciences and as an Assistant Professor at the Faculty of Mathematics and Information Sciences, Warsaw University of Technology. His research interests include feature selection in high-dimensional supervised problems, multi-label classification, learning from partially labelled data, learning under distribution shift and applications of machine learning methods in medicine and genetics.
  co-authors: Jan Mielniczuk
  date: Friday / 17 October
  time: 10:00 - 10:30
  room: Hall A
  session: CfC Session 7
  id: 20
  author-image: images/optimized/cfc-600x600/pawel_teisseyre.webp

- author-name: Jan Mielniczuk
  title: "Joint empirical risk minimization for instance-dependent positive-unlabeled data"
  author-title: "Polish Academy of Sciences"
  abstract: >- 
    Learning from positive and unlabeled data (PU learning) is actively researched machine learning task. The goal is to train a binary classification model based on a training dataset containing part of positives which are labeled, and unlabeled instances. Unlabeled set includes remaining part of positives and all negative observations. An important element in PU learning is modeling of the labeling mechanism, i.e. labels’ assignment to positive observations. Unlike in many prior works, we consider a realistic setting for which probability of label assignment, i.e. propensity score, is instance-dependent. In our approach we investigate minimizer of an empirical counterpart of a joint risk which depends on both posterior probability of inclusion in a positive class as well as on a propensity score. The non-convex empirical risk is alternately optimised with respect to parameters of both functions. In the theoretical analysis we establish risk consistency of the minimizers using recently derived methods from the theory of empirical processes. Besides, the important development here is a proposed novel implementation of an optimisation algorithm, for which sequential approximation of a set of positive observations among unlabeled ones is crucial. This relies on modified technique of ’spies’ as well as on a thresholding rule based on conditional probabilities. Experiments conducted on 20 data sets for various labeling scenarios show that the proposed method works on par or more effectively than state-of-the-art methods based on propensity function estimation.
  author-bio: >- 
    Jan Mielniczuk is a full professor at the Institute of Computer Science, Polish Academy of Sciences, and professor at the Faculty of Mathematics and Information Sciences of Warsaw University of Technology.  His main research contributions concern computational statistics and data mining, in particular time series modeling and prediction, inference for high dimensional and misspecified data, model selection, computer-intensive methods, asymptotic analysis, and quantification of dependence. He is an author and co-author of two books and over ninety articles.
  co-authors: Paweł Teisseyre, Wojciech Rejchel
  date: Friday / 17 October
  time: 10:30 - 11:00
  room: Hall A
  session: CfC Session 7
  id: 21
  author-image: images/optimized/cfc-600x600/jan_mielniczuk.webp

- title: "Limits in Causal Discovery and the Path Forward"
  authors:
    - name: Mateusz Gajewski
      title: Poznan University of Technology, IDEAS NCBR
      image: images/optimized/cfc-600x600/mateusz_gajewski.webp
    - name: Mateusz Olko
      title: University of Warsaw, IDEAS NCBR
      image: images/optimized/tutorials-600x600/Mateusz_Olko.webp
  abstract: >- 
    Causal discovery—inferring causal relationships from observational data—is of fundamental importance across many scientific fields, from understanding gene regulatory networks in biology to analyzing economic systems and climate dynamics. Real-world causal relationships are typically nonlinear and complex, making neural network-based approaches particularly appealing for their expressiveness and scalability.  Recent developments in neural causal discovery have focused on scaling methods to higher dimensions, with approaches like NOTEARS, DiBS, and BayesDAG promising to handle hundreds of variables with complex nonlinear relationships. However, almost all these methods rely on the faithfulness assumption—that conditional independencies in data correspond to causal structure in the underlying graph. While theoretical results for linear systems have shown that faithfulness becomes increasingly violated as graph complexity grows, the implications for nonlinear neural methods remain unclear. We designed carefully controlled experiments to evaluate faithfulness violations in the nonlinear setting that practitioners actually use.  Our results from an article  “Since Faithfulness Fails: The Performance Limits of Neural Causal Discovery” (published in ICML 2025) confirm the theoretical predictions: we demonstrate that reliable causal discovery would require exponentially many data points as graph size and density increase.  This fundamental limitation suggests the need for alternative paradigms. We discuss promising future directions:  - Amortized approaches: Learning to perform causal discovery across multiple related datasets. - Partial graph discovery: Targeting specific causal questions rather than complete structure recovery. - Grounding methods evaluations in real world systems: Ground-truth graphs are usually not known in real world scenarios, however it is still unknown what characteristics do real world systems exhibit.   This work highlights fundamental constraints in current causal discovery paradigms while pointing toward more realistic and achievable objectives for automated causal reasoning systems.
  author-bio: |- 
    Mateusz Gajewski is a PhD student at Poznan University of Technology and IDEAS NCBR. His research interests include causal inference (with a primary focus on causal discovery) and explainability in machine learning, particularly through game-theoretic approaches.
    <br><br>
    Mateusz Olko is a doctoral researcher at the University of Warsaw and IDEAS NCBR. He earned both his Bachelor’s and Master’s degrees in Computer Science from the University of Warsaw, specializing in machine learning. He explores topics in causal machine learning and causal discovery, with a particular interest in how they can be connected to deep learning. More broadly, he is interested in computational reasoning and how learning systems can better capture structure and support decision-making. His research has been recognized at top-tier AI conferences, including NeurIPS and ICML.
  date: Friday / 17 October
  time: 9:30 - 10:00
  room: Hall B
  session: CfC Session 8
  id: 22

- author-name: Paweł Morzywołek
  title: "Inference on Local Variable Importance Measures for Heterogeneous Treatment Effects"
  author-title: "University of Washington"
  abstract: >- 
    We provide an inferential framework to assess variable importance for heterogeneous treatment effects. This assessment is especially useful in high-risk domains such as medicine, where decision makers hesitate to rely on black-box treatment recommendation algorithms. The variable importance measures we consider are local in that they may differ across individuals, while the inference is global in that it tests whether a given variable is important for any individual. Our approach builds on recent developments in semiparametric theory for function-valued parameters, and is valid even when statistical machine learning algorithms are employed to quantify treatment effect heterogeneity. We demonstrate the applicability of our methodology in the context of infectious disease prevention strategies.
  author-bio: >- 
    I am a postdoc in the Department of Statistics at the University of Washington. My research focuses on causal inference and statistical inference for infinite-dimensional parameters, with application to study the efficacy of infectious disease prevention strategies.
  co-authors: Peter Gilbert, Alex Luedtke
  date: Friday / 17 October
  time: 10:00 - 10:30
  room: Hall B
  session: CfC Session 8
  id: 23
  author-image: images/optimized/cfc-600x600/pawel_morzywolek.webp

- author-name: Michael Vollenweider
  title: "Learning Personalized Treatment Decisions in Precision Medicine: Disentangling Treatment Assignment Bias in Counterfactual Outcome Prediction and Biomarker Identification"
  author-title: "ETH Zurich"
  abstract: >- 
    (ML4H 2024 paper: arXiv:2410.00509) Precision medicine has the potential to tailor treatment decisions to individual patients using machine learning (ML) and artificial intelligence (AI), but it faces significant challenges due to complex biases in clinical observational data and the high-dimensional nature of biological data. This study models various types of treatment assignment biases using mutual information and investigates their impact on ML models for counterfactual prediction and biomarker identification. Unlike traditional counterfactual benchmarks that rely on fixed treatment policies, our work focuses on modeling different characteristics of the underlying observational treatment policy in distinct clinical settings. We validate our approach through experiments on toy datasets, semi-synthetic tumor cancer genome atlas (TCGA) data, and real-world biological outcomes from drug and CRISPR screens. By incorporating empirical biological mechanisms, we create a more realistic benchmark that reflects the complexities of real-world data. Our analysis reveals that different biases lead to varying model performances, with some biases, especially those unrelated to outcome mechanisms, having minimal effect on prediction accuracy. This highlights the crucial need to account for specific biases in clinical observational data in counterfactual ML model development, ultimately enhancing the personalization of treatment decisions in precision medicine.
  author-bio: >- 
    Michael Vollenweider is a Data Science master’s student at ETH Zurich and will soon start hid PhD at the statistics department. His work sits at the intersection of machine learning, causality, and biology, developing methods to learn from high-dimensional biological data.
  co-authors: Manuel Schürch, Chiara Rohrer, Gabriele Gut, Michael Krauthammer, Andreas Wicki
  date: Friday / 17 October
  time: 10:30 - 11:00
  room: Hall B
  session: CfC Session 8
  id: 24
  author-image: images/optimized/cfc-600x600/michael_vollenweider.webp
